{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of clustering \n",
    "\n",
    "## Purity\n",
    "Purity is a simple and transparent evaluation measure. Purity will measure how accurately the points are clustered.\n",
    "\n",
    "$$ Purity(\\Omega) = \\frac {1}{m} \\sum_j {max_i{(\\omega_{ij})}} $$\n",
    "\n",
    "where $\\Omega = \\{ \\omega_1, \\omega_2, \\ldots, \\omega_j \\}$is the set of clusters and $\\mathbb{C} = \\{ c_1,c_2,\\ldots,c_J \\}$ is the set of classes, and m is the total number of points.\n",
    "\n",
    "High purity is easy to achieve when the number of clusters is large - in particular, purity is 1 if each document gets its own cluster. Thus, we cannot use purity to trade off the quality of the clustering against the number of clusters. \n",
    "\n",
    "## entropy\n",
    "\n",
    "###  Cluster Entropy\n",
    "\n",
    "This parameter checks the homogeneity of clusters. The entropy of cluster j is defined by\n",
    "\n",
    "$$ H(\\omega_j) = -\\sum_i(\\frac {c(i,j)}{\\sum_i c(i,j)}).log(\\frac {c(i,j)}{\\sum_i c(i,j)})$$\n",
    "\n",
    "Where c(i, j) is the number of times label i occurs in cluster j.\n",
    "    \n",
    "The entropy for a cluster is zero if the labels of all the documents are the same, otherwise it is positive.\n",
    "\n",
    " **The total cluster entropy** is the weighted average of the individual cluster entropies:\n",
    "\n",
    "$$ H(\\Omega) = \\frac {1}{m} \\sum_j {e_j} . ({\\sum_i c(i,j)}) $$\n",
    "\n",
    "Where ${\\sum_i c(i,j)}$ is the number of documents in cluster j.\n",
    "\n",
    "As a consequence, the lower the entropy the better the quality.\n",
    "\n",
    "### Class Entropy\n",
    "\n",
    "Class entropy evaluates how cluster fragmentation rate. Optimal value of class entropy is zero.\n",
    "If a single topic is present in multiple clusters, it increases the value of class entropy.\n",
    "$$ e_i = -\\sum_j{(\\frac {c(i,j)}{\\sum_j c(i,j)})}.log(\\frac {c(i,j)}{\\sum_j c(i,j)})$$\n",
    " **The total class entropy** is the weighted average of the individual cluster entropies:\n",
    "\n",
    "$$ e_{total} = \\frac {1}{m} \\sum_i {e_i} . ({\\sum_j c(i,j)})$$"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity(Clusters):\n",
    "    m = 0\n",
    "    p = 0\n",
    "    for Cluster in Clusters:\n",
    "        m += sum(Cluster)\n",
    "        p += max(Cluster)\n",
    "    P = p / m        \n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "def entropy(Cluster):\n",
    "    N_c = sum(Cluster)\n",
    "    e = 0\n",
    "    for Point in Cluster:\n",
    "        if Point > 0:\n",
    "            e += (Point / N_c) * log(Point / N_c,2)\n",
    "    return -e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_total(Clusters):\n",
    "    m = 0\n",
    "    e = 0\n",
    "    e_total = 0\n",
    "    for Cluster in Clusters:\n",
    "        N_c = sum(Cluster)\n",
    "        m += N_c\n",
    "        e += entropy(Cluster) * (N_c)\n",
    "    e_total = e / m\n",
    "    return e_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Cluster\n",
    "\n",
    "\n",
    "![\"Fig 1\"](img\\ClusterExample.jpg)\n",
    "\n",
    "*Fig. 1: Cluster Example*\n",
    "\n",
    "class:\n",
    "- class 1: X\n",
    "- class 2: O\n",
    "- class 3: D\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C = {X,O,D}\n",
    "C = [[5,1,0],\n",
    "     [1,4,1],\n",
    "     [2,0,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of the Purity\n",
    "\n",
    "$Purity = (\\frac{1}{17}) \\times (5+4+3) = 0.70588 $"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = purity(C)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of the Cluster entropy \n",
    "\n",
    "$ H(\\omega_1) = (\\frac{5}{6}) \\times \\log_2(\\frac {5}{6}) + \n",
    "                (\\frac{1}{6}) \\times \\log_2(\\frac {1}{6}) + \n",
    "                (\\frac{0}{6}) \\times \\log_2(\\frac{0}{6}) = 0.650 $\n",
    "\n",
    "$ H(\\omega_2) = (\\frac{1}{6}) \\times \\log_2(\\frac {1}{6}) + \n",
    "                (\\frac{4}{6}) \\times \\log_2(\\frac {4}{6}) + \n",
    "                (\\frac{1}{6}) \\times \\log_2(\\frac {1}{6}) = 1.252 $\n",
    "\n",
    "$ H(\\omega_3) = (\\frac{2}{5}) \\times \\log_2(\\frac {2}{5}) +\n",
    "                (\\frac{0}{5}) \\times \\log_2(\\frac {0}{5}) +\n",
    "                (\\frac{3}{5}) \\times \\log_2(\\frac {3}{5}) = 0.971 $\n",
    "\n",
    "So then to find the total entropy for a set of clusters, you take the sum of the entropies times the relative weight of each cluster.\n",
    "\n",
    "$ H(\\Omega) = (0.650 \\times \\frac {6}{17} + 1.252 \\times \\frac {6}{17} + 0.971 \\times \\frac {5}{17}) $\n",
    "\n",
    "$ H(\\Omega) = 0.956 $\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = entropy_total(C)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of the Class entropy\n",
    "\n",
    "$C = [[5,1,0],\n",
    "     [1,4,1],\n",
    "     [2,0,3]]$\n",
    "\n",
    "$C_T = [[5,1,2],\n",
    "       [1,4,0],\n",
    "       [0,1,3]]$\n",
    "\n",
    "$ e(c_1) = (\\frac{5}{8}) \\times \\log_2(\\frac {5}{8}) + \n",
    "           (\\frac{1}{8}) \\times \\log_2(\\frac {1}{8}) + \n",
    "           (\\frac{2}{8}) \\times \\log_2(\\frac {2}{8}) = 1.298 $\n",
    "\n",
    "$ e(c_2) = (\\frac{1}{5}) \\times \\log_2(\\frac {1}{5}) + \n",
    "           (\\frac{4}{5}) \\times \\log_2(\\frac {4}{5}) + \n",
    "           (\\frac{0}{5}) \\times \\log_2(\\frac {0}{5}) = 0.721 $\n",
    "\n",
    "$ e(c_3) = (\\frac{0}{4}) \\times \\log_2(\\frac {0}{4}) +\n",
    "           (\\frac{1}{4}) \\times \\log_2(\\frac {1}{4}) +\n",
    "           (\\frac{3}{5}) \\times \\log_2(\\frac {3}{4}) = 0.811 $\n",
    "\n",
    "So then to find the total entropy for a set of classes, you take the sum of the entropies times the relative weight of each class.\n",
    "\n",
    "$ e_{total}(\\mathbb{C}) = (1.298 \\times \\frac {8}{17} + 0.721 \\times \\frac {5}{17} + 0.811 \\times \\frac {4}{17}) $\n",
    "\n",
    "$ e_{total}(\\mathbb{C}) = 1.011 $\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "C = np.array(C)\n",
    "C_T = C.transpose()\n",
    "\n",
    "H = entropy_total(C_T)\n",
    "print(H)\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}